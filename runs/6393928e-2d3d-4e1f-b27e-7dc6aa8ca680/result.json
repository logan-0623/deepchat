{"status": "success", "type": "pdf", "task_id": "6393928e-2d3d-4e1f-b27e-7dc6aa8ca680", "file_name": "6393928e-2d3d-4e1f-b27e-7dc6aa8ca680_6baaf0a6f27417dea8da52e73ac0032d.pdf", "content": "\n--- 第 1 页 ---\nLecture 3 -Learning BasicsDr. Xiaowei Huanghttps://cgi.csc.liv.ac.uk/~xiaowei/ai.html(Attendance Code: 739876)\n--- 第 2 页 ---\nIn the last lecture, \nWhat is machine learning? \nA few applications of machine learning\nconsider how to represent instances as fixed-length feature vectors \n--- 第 3 页 ---\nTopics \n•Learning basics: •Before learning: data collection•Learning tasks: supervised and unsupervised learning•Learning schemes\n--- 第 4 页 ---\nBefore learning: data collection\n--- 第 5 页 ---\nIndependent and identically distributed (i.i.d.)\n•we often assume that training instances are independent and identically distributed (i.i.d.) – sampled independently from the same unknown distribution •there are also cases where this assumption does not hold •cases where sets of instances have dependencies •instances sampled from the same medical image •instances from time series•etc. \n--- 第 6 页 ---\nLearning tasks: supervised and unsupervised learning\n--- 第 7 页 ---\nThree canonical learning problems \n\n--- 第 8 页 ---\nThree canonical learning problems \n\n--- 第 9 页 ---\nThe supervised learning task •problem setting •set of possible instances: X •unknown target function: •set of models (a.k.a. hypotheses): •given training set of instances of unknown target function f •Output•model    that best approximates target function \nf(x) = w1x+w2f(x) = (w1x+w2)>10 \n--- 第 10 页 ---\nThe supervised learning task •when y is discrete, we term this a classification task (or concept learning) •when y is continuous, it is a regression task •there are also tasks in which each y is more structured object like a sequence of discrete labels (as in e.g. image segmentation, machine translation) f(x) = w1x+w2f(x) = (w1x+w2)>10 \n\n--- 第 11 页 ---\nModel representations •throughout the semester, we will consider a broad range of representations for learned models, including •decision trees •neural networks •Linear/logistic regression •Bayesian networks •etc. Program or ModelTraining dataLearning AlgorithmProgram’ or Model’\n\n--- 第 12 页 ---\nMushroom features (from the UCI Machine Learning Repository) \n\n--- 第 13 页 ---\nA learned decision tree \n\n--- 第 14 页 ---\nClassification with a learned decision tree \nx = <bell,fibrous,brown,false, foul,...> y = ?y=p\n--- 第 15 页 ---\nUnsupervised learning •in unsupervised learning, we’re given a set of instances, without y’s                                       x(1),x(2) ... x(m)goal: discover interesting regularities/structures/patterns that characterize the instances •common unsupervised learning tasks•clustering •anomaly detection •dimensionality reduction \n--- 第 16 页 ---\nClustering •given •training set of instances x(1) , x(2) ... x(m) •output •model that divides the training set into clusters such that there is intra-cluster similarity and inter-cluster dissimilarity \n\n--- 第 17 页 ---\nClustering example \n\n--- 第 18 页 ---\nSemi-Supervised Learning•using labelled as well as unlabelled data to perform certain learning tasks \n\n--- 第 19 页 ---\nAnomaly detection \n\n--- 第 20 页 ---\nAnomaly detection example \nLet’s say our model is represented by: 1979-2000 average, ±2 stddev. Does the data for 2012 look anomalous?\n--- 第 21 页 ---\nDimensionality reduction •given •training set of instances x(1) , x(2) ... x(m) •output •Model    that represents each x with a lower-dimension feature vector while still preserving key properties of the data \n\n--- 第 22 页 ---\nDimensionality reduction example \nWe can represent a face using all of the pixels in a given image More effective method (for many tasks): represent each face as a linear combination of eigenfaces \n--- 第 23 页 ---\nDimensionality reduction example •represent each face as a linear combination of eigenfaces \n•# of features is now 20 instead of # of pixels in images \n\n--- 第 24 页 ---\nOther learning tasks •later in the semester we’ll cover other learning tasks that are not strictly supervised or unsupervised •reinforcement learning •transfer learning•etc. \n\n--- 第 25 页 ---\nLearning Schemes\n--- 第 26 页 ---\nBatch vs. online learning •In batch learning, the learner is given the training                                   set as a batch (i.e. all at once) •In online learning, the learner receives instances sequentially, and updates the model after each (for some tasks it might have to classify/make a prediction for each x(i) before seeing y(i) ) \n\n--- 第 27 页 ---\nActive learning and concept drift\nActive learning: cases where the learner can select which instances for training the target function changes over time (concept drift) \n--- 第 28 页 ---\nGeneralization \n•The primary objective in supervised learning is to find a model that generalizes •one that accurately predicts y for previously unseen x \nCan I eat this mushroom that was not in my training set? "}