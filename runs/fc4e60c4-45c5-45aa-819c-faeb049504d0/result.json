{"status": "success", "type": "pdf", "task_id": "fc4e60c4-45c5-45aa-819c-faeb049504d0", "file_name": "fc4e60c4-45c5-45aa-819c-faeb049504d0_3b5efbd98cab6ef8b0bbd9bd74c1f078.pdf", "content": "\n--- ç¬¬ 1 é¡µ ---\nÂ© 2019 McGraw-Hill Education\nSichen LiuSichen.Liu@xjtlu.edu.cnINT104 ARTIFICIAL INTELLIGENCELECTURE 3-DIMENSIONALITYREDUCTION\n1\n--- ç¬¬ 2 é¡µ ---\nÂ© 2019 McGraw-Hill Education\nCONTENTÃ˜Why need Dimensionality ReductionÃ˜Principal Component Analysis (PCA)Ã˜Locally Linear Embedding (LLE)Ã˜Other Dimensionality Reduction Techniques\n2\n--- ç¬¬ 3 é¡µ ---\nÂ© 2019 McGraw-Hill Education\nDimensionality ReductionData with high dimensions:â€¢High computational complexity â€¢May contain many irrelevant or redundant featuresâ€¢Difficulty in visualizationâ€¢With high risk of getting an overfitting model\n3\n--- ç¬¬ 4 é¡µ ---\nÂ© 2019 McGraw-Hill Education\nApproaches for Dimensionality ReductionProjection:â€¢Data is not spread out uniformly across all dimensions. (All the data lies within (or close to) a much lower-dimensional subspace of the high-dimensional space.\n4\n--- ç¬¬ 5 é¡µ ---\nÂ© 2019 McGraw-Hill Education\nPrincipal Component Analysis (PCA)Preserving the Variance:\nPCA identifies the axis that accounts for the largest amount of variance in the training set.\n5C3C1C2C3\n--- ç¬¬ 6 é¡µ ---\nÂ© 2019 McGraw-Hill Education\nPrincipal Component Analysis (PCA)-Variance on C1ğ‘‰!=1ğ‘€%\"#!$(ğ’„!âŠºğ’™(\"))(=1ğ‘€%\"#!$ğ’„!âŠºğ’™(\")ğ’™(\")âŠºğ’„!=ğ’„!âŠº(1ğ‘€%\"#!$ğ’™(\")ğ’™(\")âŠº)ğ’„!=ğ’„!âŠºğ‘†ğ’„!-Data covariance matrixğ‘†=1ğ‘€%\"#!$ğ’™(\")ğ’™(\")âŠº-S is an N*N matrix, N is the number of features, M is the total number of data points. \n6\n--- ç¬¬ 7 é¡µ ---\nÂ© 2019 McGraw-Hill Education\nPrincipal Component Analysis (PCA)-Constrained optimization problemmaxğ’„!ğ’„!âŠºğ‘†ğ’„!subject to  ğ’„!(=1-Lagrange equation  â„’ğ’„!,ğœ†!=ğ’„!âŠºğ‘†ğ’„!+ğœ†!(1âˆ’ğ’„!âŠºğ’„!)-Solve this constrained optimization problem*â„’*ğ’„!=2ğ‘†ğ’„!âˆ’2ğœ†!ğ’„!*â„’*,!=1âˆ’ğ’„!âŠºğ’„!-Setting these partial derivatives to 0 gives us the relations:ğ‘†ğ’„!=ğœ†!ğ’„!and    ğ’„!âŠºğ’„!=1-Variance onC1ğ‘‰!=ğ’„!âŠºğ‘†ğ’„!=ğœ†!ğ’„!âŠºğ’„!=ğœ†!7\n--- ç¬¬ 8 é¡µ ---\nÂ© 2019 McGraw-Hill Education\nPractice: PCAGiven a dataset that consists of the following points below:A=(2, 3), B=(5, 5), C=(6, 6), D=(8,9)1. Calculate the covariance matrix for the dataset.2. Calculate the eigenvalues and eigenvectors of the covariance matrix.\n8\n--- ç¬¬ 9 é¡µ ---\nÂ© 2019 McGraw-Hill Education\nPCASingular Value Decomposition (SVD)Theorem: Let ğ´âˆˆğ‘…-âˆ—/be a rectangular matrix of rank r âˆˆ[0, min(m, n)]. The SVD of A is a decomposition of the form-ğ‘ˆâˆˆğ‘…-âˆ—-is an orthogonal matrix with column vectors ğ‘¢\",ğ‘–=1,...ğ‘š,-ğ‘‰âˆˆğ‘…/âˆ—/an orthogonal matrixwith column vectors ğ‘£0,ğ‘—=1,...ğ‘›.-Î£ is an m Ã—n matrix with ğ›´\"\"=ğœ\"â‰¥0and ğ›´\"0=0,ğ‘–â‰ ğ‘—-ğ‘»ğ’‰ğ’†ğ’”ğ’Šğ’ğ’ˆğ’–ğ’ğ’‚ğ’“ğ’—ğ’‚ğ’ğ’–ğ’†ğ’ğ’‚ğ’•ğ’“ğ’Šğ’™ğœ®ğ’Šğ’”ğ’–ğ’ğ’Šğ’’ğ’–ğ’†\n9\n--- ç¬¬ 10 é¡µ ---\nÂ© 2019 McGraw-Hill Education\nPCASingular Value Decomposition (SVD)ğ´=ğ‘¥!...ğ‘¥\"#âˆ—\"=ğ‘ˆğ›´ğ‘‰âŠº=ğ‘¢!...ğ‘¢##âˆ—#ğœ!000â‹±000ğœ\"0â€¦0â‹®â‹®0â€¦0#âˆ—\"ğ‘£!â€¦ğ‘£\"\"âˆ—\"âŠºV contains the unit vectors that define all the principal components that we are looking for.\n10\n\n--- ç¬¬ 11 é¡µ ---\nÂ© 2019 McGraw-Hill Education\nPrincipal Component Analysis (PCA)Principal components matrixğ’„!,ğ’„(â€¦ğ’„/are orthogonalProjecting Down to d Dimension:ğ‘‰?is the first d eigen vectors of data covariance matrixExplained Variance Ratio,!,!@,\"â€¦@,#(eigenvalue/ total eigenvalue)ğ‘‹!\"#$%&=ğ‘‹ğ‘‰!\n11\n--- ç¬¬ 12 é¡µ ---\nÂ© 2019 McGraw-Hill Education\nPCAChoosing the Right Number of Dimensions:â€¢Choose the number of dimensions that add up to sufficiently large portion of the variance (e.g., 95%)â€¢,!@â‹¯@,$,!@,\"â€¦@,#>95%\n12\n\n--- ç¬¬ 13 é¡µ ---\nÂ© 2019 McGraw-Hill Education\nPCAKey steps of PCA in practice \n13\n--- ç¬¬ 14 é¡µ ---\nÂ© 2019 McGraw-Hill Education\nPCAPCA for Compressionâ€¢Projecting Down to d Dimensionâ€¢PCA inverse transformation, back to the original number of dimensions\n(Example MNIST data: 40% original size preserves 95% variance)ğ‘‹!\"#$%&=ğ‘‹ğ‘‰!ğ‘‹$'(%)'$'!=ğ‘‹!\"#$%&ğ‘‰!âŠº\n14\n--- ç¬¬ 15 é¡µ ---\nÂ© 2019 McGraw-Hill Education\nApproaches for Dimensionality ReductionManifold Learningâ€¢Data lies on d-dimensional manifold is a part of an n-dimensional space (where d < n)\nSimply projecting onto a planeUnrolling the Swiss roll15\n--- ç¬¬ 16 é¡µ ---\nÂ© 2019 McGraw-Hill Education\nLocally Linear Embedding (LLE)LLE is a powerful nonlinear dimensionality reduction (NLDR) technique. It is a Manifold Learning technique that does not rely on projections Step one: Linearly modeling local relationshipsStep two: Reducing dimensionality while preserving relationships\n16\n--- ç¬¬ 17 é¡µ ---\nÂ© 2019 McGraw-Hill Education\nLocally Linear Embedding (LLE)\n17\n--- ç¬¬ 18 é¡µ ---\nÂ© 2019 McGraw-Hill Education\nOther Techniques\nâ€¢Multidimensional Scaling (MDS)Trying to preserve the distances between the instances.â€¢IsomapTrying to preserve the geodesic distances between the instances.â€¢t-Distributed Stochastic Neighbor Embedding (t-SNE)Trying to keep similar instances close and dissimilar instances apart. \n18\n--- ç¬¬ 19 é¡µ ---\nÂ© 2019 McGraw-Hill Education\nTHANK YOU"}