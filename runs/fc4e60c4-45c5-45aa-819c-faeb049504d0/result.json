{"status": "success", "type": "pdf", "task_id": "fc4e60c4-45c5-45aa-819c-faeb049504d0", "file_name": "fc4e60c4-45c5-45aa-819c-faeb049504d0_3b5efbd98cab6ef8b0bbd9bd74c1f078.pdf", "content": "\n--- 第 1 页 ---\n© 2019 McGraw-Hill Education\nSichen LiuSichen.Liu@xjtlu.edu.cnINT104 ARTIFICIAL INTELLIGENCELECTURE 3-DIMENSIONALITYREDUCTION\n1\n--- 第 2 页 ---\n© 2019 McGraw-Hill Education\nCONTENTØWhy need Dimensionality ReductionØPrincipal Component Analysis (PCA)ØLocally Linear Embedding (LLE)ØOther Dimensionality Reduction Techniques\n2\n--- 第 3 页 ---\n© 2019 McGraw-Hill Education\nDimensionality ReductionData with high dimensions:•High computational complexity •May contain many irrelevant or redundant features•Difficulty in visualization•With high risk of getting an overfitting model\n3\n--- 第 4 页 ---\n© 2019 McGraw-Hill Education\nApproaches for Dimensionality ReductionProjection:•Data is not spread out uniformly across all dimensions. (All the data lies within (or close to) a much lower-dimensional subspace of the high-dimensional space.\n4\n--- 第 5 页 ---\n© 2019 McGraw-Hill Education\nPrincipal Component Analysis (PCA)Preserving the Variance:\nPCA identifies the axis that accounts for the largest amount of variance in the training set.\n5C3C1C2C3\n--- 第 6 页 ---\n© 2019 McGraw-Hill Education\nPrincipal Component Analysis (PCA)-Variance on C1𝑉!=1𝑀%\"#!$(𝒄!⊺𝒙(\"))(=1𝑀%\"#!$𝒄!⊺𝒙(\")𝒙(\")⊺𝒄!=𝒄!⊺(1𝑀%\"#!$𝒙(\")𝒙(\")⊺)𝒄!=𝒄!⊺𝑆𝒄!-Data covariance matrix𝑆=1𝑀%\"#!$𝒙(\")𝒙(\")⊺-S is an N*N matrix, N is the number of features, M is the total number of data points. \n6\n--- 第 7 页 ---\n© 2019 McGraw-Hill Education\nPrincipal Component Analysis (PCA)-Constrained optimization problemmax𝒄!𝒄!⊺𝑆𝒄!subject to  𝒄!(=1-Lagrange equation  ℒ𝒄!,𝜆!=𝒄!⊺𝑆𝒄!+𝜆!(1−𝒄!⊺𝒄!)-Solve this constrained optimization problem*ℒ*𝒄!=2𝑆𝒄!−2𝜆!𝒄!*ℒ*,!=1−𝒄!⊺𝒄!-Setting these partial derivatives to 0 gives us the relations:𝑆𝒄!=𝜆!𝒄!and    𝒄!⊺𝒄!=1-Variance onC1𝑉!=𝒄!⊺𝑆𝒄!=𝜆!𝒄!⊺𝒄!=𝜆!7\n--- 第 8 页 ---\n© 2019 McGraw-Hill Education\nPractice: PCAGiven a dataset that consists of the following points below:A=(2, 3), B=(5, 5), C=(6, 6), D=(8,9)1. Calculate the covariance matrix for the dataset.2. Calculate the eigenvalues and eigenvectors of the covariance matrix.\n8\n--- 第 9 页 ---\n© 2019 McGraw-Hill Education\nPCASingular Value Decomposition (SVD)Theorem: Let 𝐴∈𝑅-∗/be a rectangular matrix of rank r ∈[0, min(m, n)]. The SVD of A is a decomposition of the form-𝑈∈𝑅-∗-is an orthogonal matrix with column vectors 𝑢\",𝑖=1,...𝑚,-𝑉∈𝑅/∗/an orthogonal matrixwith column vectors 𝑣0,𝑗=1,...𝑛.-Σ is an m ×n matrix with 𝛴\"\"=𝜎\"≥0and 𝛴\"0=0,𝑖≠𝑗-𝑻𝒉𝒆𝒔𝒊𝒏𝒈𝒖𝒍𝒂𝒓𝒗𝒂𝒍𝒖𝒆𝒎𝒂𝒕𝒓𝒊𝒙𝜮𝒊𝒔𝒖𝒏𝒊𝒒𝒖𝒆\n9\n--- 第 10 页 ---\n© 2019 McGraw-Hill Education\nPCASingular Value Decomposition (SVD)𝐴=𝑥!...𝑥\"#∗\"=𝑈𝛴𝑉⊺=𝑢!...𝑢##∗#𝜎!000⋱000𝜎\"0…0⋮⋮0…0#∗\"𝑣!…𝑣\"\"∗\"⊺V contains the unit vectors that define all the principal components that we are looking for.\n10\n\n--- 第 11 页 ---\n© 2019 McGraw-Hill Education\nPrincipal Component Analysis (PCA)Principal components matrix𝒄!,𝒄(…𝒄/are orthogonalProjecting Down to d Dimension:𝑉?is the first d eigen vectors of data covariance matrixExplained Variance Ratio,!,!@,\"…@,#(eigenvalue/ total eigenvalue)𝑋!\"#$%&=𝑋𝑉!\n11\n--- 第 12 页 ---\n© 2019 McGraw-Hill Education\nPCAChoosing the Right Number of Dimensions:•Choose the number of dimensions that add up to sufficiently large portion of the variance (e.g., 95%)•,!@⋯@,$,!@,\"…@,#>95%\n12\n\n--- 第 13 页 ---\n© 2019 McGraw-Hill Education\nPCAKey steps of PCA in practice \n13\n--- 第 14 页 ---\n© 2019 McGraw-Hill Education\nPCAPCA for Compression•Projecting Down to d Dimension•PCA inverse transformation, back to the original number of dimensions\n(Example MNIST data: 40% original size preserves 95% variance)𝑋!\"#$%&=𝑋𝑉!𝑋$'(%)'$'!=𝑋!\"#$%&𝑉!⊺\n14\n--- 第 15 页 ---\n© 2019 McGraw-Hill Education\nApproaches for Dimensionality ReductionManifold Learning•Data lies on d-dimensional manifold is a part of an n-dimensional space (where d < n)\nSimply projecting onto a planeUnrolling the Swiss roll15\n--- 第 16 页 ---\n© 2019 McGraw-Hill Education\nLocally Linear Embedding (LLE)LLE is a powerful nonlinear dimensionality reduction (NLDR) technique. It is a Manifold Learning technique that does not rely on projections Step one: Linearly modeling local relationshipsStep two: Reducing dimensionality while preserving relationships\n16\n--- 第 17 页 ---\n© 2019 McGraw-Hill Education\nLocally Linear Embedding (LLE)\n17\n--- 第 18 页 ---\n© 2019 McGraw-Hill Education\nOther Techniques\n•Multidimensional Scaling (MDS)Trying to preserve the distances between the instances.•IsomapTrying to preserve the geodesic distances between the instances.•t-Distributed Stochastic Neighbor Embedding (t-SNE)Trying to keep similar instances close and dissimilar instances apart. \n18\n--- 第 19 页 ---\n© 2019 McGraw-Hill Education\nTHANK YOU"}