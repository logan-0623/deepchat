{"status": "success", "type": "pdf", "task_id": "62d16a40-4df7-42d8-a65e-c9458085c08d", "file_name": "62d16a40-4df7-42d8-a65e-c9458085c08d_2fb341757634066347b72dec58b9870b.pdf", "content": "\n--- 第 1 页 ---\nLecture 4 - Probability Foundation and Linear Algebra for Machine LearningProf. Xiaowei Huanghttps://cgi.csc.liv.ac.uk/~xiaowei/ (Attendance Code: 641014)\n--- 第 2 页 ---\nIn the last week’s lectures, \nModule contents\nA few applications of machine learningRepresentation of instances as vectorsData preprocessing \nLearning basicsSupervised, unsupervised, semi-supervised learning\n--- 第 3 页 ---\nToday ’s Topics•Joint probability, conditional probability, etcProbability distribution•Probability query•MAP Two types of probabilistic queries•Scalars, vectors, matrices, tensors •Multiplying matrices/vectorsLinear algebra•Numpy•Scipy•MatplotlibIntroduction to Scientific Python \n--- 第 4 页 ---\nProbability foundations \nRandom Variables\nJoint and Conditional Distributions\nIndependence and Conditional Independence\nQuerying Joint Probability Distributions Probability queryMAP query Machine Learning is about the measurement of, and the  reasoning about, probabilistic distributions.\n--- 第 5 页 ---\nRandom Variables\n--- 第 6 页 ---\nRandom Variable We have a population of students •We want to reason about their grades •Random variable: Grade •P(Grade) associates a probability with each outcome Val(Grade)={ A, B, C }If k=|Val{X}| then  •Distribution is referred to as a multinomial •If Val{X}={false,true} then it is a Bernoulli distribution P(X) is known as the marginal distribution of X\nUse RVs to express uncertainty etc\n--- 第 7 页 ---\nJoint and Conditional Distributions\n\n--- 第 8 页 ---\nRecap: Marginal, joint, conditional probability•Marginal probability: the probability of an event occurring P(A), it may be thought of as an unconditional probability. It is not conditioned on another event. •Example: the probability that a card drawn is red, i.e., P(red) = 0.5. •Another example: the probability that a card drawn is a 4, i.e., P(four)=1/13.•Joint probability: P(A and B). The probability of event Aandevent B occurring. It is the probability of the intersection of two or more events. The probability of the intersection of A and B may be written P(A ∩ B). •Example: the probability that a card is a four and red, i.e., P(four and red) = 2/52=1/26. (There are two red fours in a deck of 52, the4 of heartsand the4 of diamonds).•Conditional probability: P(A|B) is the probability of event A occurring, given that event B occurs. •Example: given that you drew a red card, what’s the probability that it’s a four, i.e., P(four|red)=2/26=1/13. So out of the 26 red cards (given a red card), there are two fours so 2/26=1/13.\n--- 第 9 页 ---\nJoint Distribution •We are interested in questions involving several random variables •Example event: Intelligence=high and Grade=A •Need to consider joint distributions •Over a set χ={X1,..,Xn} denoted by P(X1,..,Xn) •We use ξ to refer to a full assignment to variables χ, i.e. ξ ε Val(χ) •Example of joint distribution•and marginal distributions \n\n--- 第 10 页 ---\nConditional Probability •P(Intelligence|Grade=A) describes the distribution over events describable by Intelligence given the knowledge that student’s grade is A •It is not the same as the marginal distribution \n\n--- 第 11 页 ---\nIndependence and Conditional Independence\n--- 第 12 页 ---\nRecap: Chain Rules\nchain rule(also called thegeneral product rule) permits the calculation of any member of thejoint distributionof a set ofrandom variablesusing onlyconditional probabilities.\n--- 第 13 页 ---\nIndependent Random Variables We expect P(α|β) to be different from P(α) •i.e., β is true changes our probability over α Sometimes equality can occur, i.e, P(α|β)=P(α) •i.e., learning that β occurs did not change our probability of αWe say event α is independent of event β,denoted •α⊥β,  if P (α|β)=P (α) or if P (β)=0 A distribution P satisfies (α⊥β) if and only if P(α∧β)=P(α)P(β) \n--- 第 14 页 ---\nConditional Independence \n•While independence is a useful property, we don’t often encounter two independent events •A more common situation is when two events are independent given an additional event •Reason about student accepted at Stanford or MIT •These two are not independent •If student admitted to Stanford then probability of MIT is higher •If both based on GPA and we know the GPA to be A •Then the student being admitted to Stanford does not change probability of being admitted to MIT •P(MIT|Stanford,Grade A)=P(MIT|Grade A) •i.e., MIT is conditionally independent of Stanford given Grade A \n--- 第 15 页 ---\nQuerying Joint Probability Distributions \n\n--- 第 16 页 ---\nQuery Types Probability Queries•Given evidence (the values of a subset of random variables), •compute distribution of another subset of random variablesMAP Queries•Maximum a posteriori probability •Also called MPE (Most Probable Explanation) •What is the most likely setting of a subset of random variables•Marginal MAP Queries•When some variables are known \n--- 第 17 页 ---\nProbability Queries -- Most common type of queries\nQuery has two parts Evidence: a subset E of variables and their instantiation e Query Variables: a subset Y of random variables  \nInference Task: P(Y|E=e) Posterior probability distribution over values y of Y Conditioned on the fact E=e Can be viewed as Marginal over Y in distribution we obtain by conditioning on e \n--- 第 18 页 ---\nProbability Queries •Marginal Probability Estimation \n\n--- 第 19 页 ---\nRecap: max and argmax\n\n--- 第 20 页 ---\nRecap: Max vs. argmax•Let x be in a range [a,b] and f be a function over [a,b], we have •max f(x) to represent the maximum value of f(x) as x varies through [a,b] •argmax f(x) to represent the value of x at which the maximum is attained•maxx sin(x)             = 1•argmaxx sin(x)             = {(0.5+2n)*pi | n is integer }            = {…, -1.5pi, 0.5pi, 2.5pi, …}\n\n--- 第 21 页 ---\nMAP Queries (Most Probable Explanation) •Finding a high probability assignment to some subset of variables •Most likely assignment to all non-evidence variables W = V–E         i.e., value of w for which P(w,e) is maximum •Difference from probability query •Instead of a probability we get the most likely value for all remaining variables \nAn example case. Other cases will be introduced later\n--- 第 22 页 ---\nExample of MAP Queries •Medical Diagnosis Problem •Diseases (A) cause Symptoms (B) •Two possible diseases: Mono and Flu •Two possible symptoms: Headache and Fever\nNotation for probabilistic graphical models, to be introduced in later part of this module\n\n--- 第 23 页 ---\nExample of MAP Queries •Medical Diagnosis Problem •Diseases (A) cause Symptoms (B) •Two possible diseases: Mono and Flu •Two possible symptoms: Headache and Fever•Q1: Most likely disease MAP (A)? •Q2: Most likely disease and symptom MAP(A,B)? •Q3: Most likely symptom MAP(B)? \n\n--- 第 24 页 ---\nExample of MAP Queries •Medical Diagnosis Problem •Diseases (A) cause Symptoms (B) •Two possible diseases: Mono and Flu •Two possible symptoms: Headache and Fever•Q1: Most likely disease MAP(A)? \n\n--- 第 25 页 ---\nExample of MAP Queries •Medical Diagnosis Problem •Diseases (A) cause Symptoms (B) •Two possible diseases: Mono and Flu •Two possible symptoms: Headache and Fever•Q2: Most likely disease and symptom P(A,B)? \n\n--- 第 26 页 ---\nExample of MAP Queries\nP(A,B) = P(B|A) P(A)\n\n--- 第 27 页 ---\nExample of MAP Queries •Medical Diagnosis Problem •Diseases (A) cause Symptoms (B) •Two possible diseases: Mono and Flu •Two possible symptoms: Headache and Fever•Q2: Most likely disease and symptom P(A,B)? \nIn this case, Y = {A,B}, E = {}, Z = {}, where Y , Z, E are defined in the next slide.\n--- 第 28 页 ---\nExample of MAP Queries •Q3: Most likely symptom MAP(B)? \n\n--- 第 29 页 ---\nMAP Query •Y is query, evidence is E=e. Task is to find most likely assignment to Y: •If Z=X-Y-E is empty •If Z=X-Y-E is not empty \nThe set X of features can be split into three subsets Y , E, and Z, where •Y is the set of variables to reason about, •E is the set of variables that we observe, and •Z is the set of variables we don’t care. \n--- 第 30 页 ---\nExample of MAP Queries •Medical Diagnosis Problem •Diseases (A) cause Symptoms (B) •Two possible diseases: Mono and Flu •Two possible symptoms: Headache and Fever •Q3: Most likely symptom P(B)? \nIn the case, Y = {B}, E = {}, and Z = {A}\n--- 第 31 页 ---\nExample of MAP Queries \n•Q3: Most likely symptom P(B)? \nP(A,B) = P(A)P(B|A)\ndefinitionmarginalisation\n--- 第 32 页 ---\nExercise 1\n0.120.180.240.060.060.090.120.030.020.030.040.01A=1A=2A=3B=1B=2B=3B=4Joint distribution table as shown rightCan you calculate the following:P(A=1) = 0.6P(A=2) = 0.3P(B=3)= 0.4P(B=4)= 0.1P(A=1|B=2)=0.6    P(B=3 | A = 3) =0.4 MAP(A | B=2) = 1MAP(B | A =2) =3 MAP(A) = 1MAP(B) = 3Check the exercises of the lecture notes for answer\n--- 第 33 页 ---\nExercise 2\n0.120.180.240.020.060.090.120.030.060.030.040.01A=1A=2A=3B=1B=2B=3B=4Joint distribution table as shown rightCan you calculate the following:P(A=1) = 0.56P(A=2) = 0.3P(B=3)= 0.4P(B=4)= 0.06P(A=1|B=2)=  0.6  P(B=3 | A = 3) =  2/7MAP(A | B=2) = 1MAP(B | A =2) = 3MAP(A) = 1MAP(B) = 3Check the exercises of the lecture notes for answer\n--- 第 34 页 ---\nLinear Algebra For Machine Learning\n\n--- 第 35 页 ---\nScalar •Single number•Represented in lower-case italic x •E.g., let   be the slope of the line •Defining a real-valued scalar •E.g., let be the number of units •Defining a natural number scalar \n\n--- 第 36 页 ---\nVector •An array of numbers •Arranged in order •Each no. identified by an index •Vectors are shown in lower-case bold •If each element is in R then x is in Rn •We think of vectors as points in space •Each element gives coordinate along an axis \n\n--- 第 37 页 ---\nMatrix •2-D array of numbers •Each element identified by two indices •Denoted by bold typeface A •Elements indicated as Am,n •E.g., •A[i:] is ith row of A, A[:j] is jth column of A •If A has shape of height m and width n with real-values then\n\n--- 第 38 页 ---\nTensor •Sometimes need an array with more than two axes (or dimensions) •An array arranged on a regular grid with variable number of axes is referred to as a tensor •Denote a tensor with bold typeface: A •Element (i,j,k) of tensor denoted by Ai,j,k Tensor is not just an array of multiple dimensions. \n--- 第 39 页 ---\nTranspose of a Matrix •Mirror image across principal diagonal •Vectors are matrices with a single column •Often written in-line using transpose •Since a scalar is a matrix with one element \n\n--- 第 40 页 ---\nLinear Transformation•where                               andn equations in n unknowns \n\n--- 第 41 页 ---\nLinear Transformation•where                           and •More explicitly \n•Sometimes we wish to solve for the unknowns x ={x1,..,xn} when A and b provide constraints \nCan view A as a linear transformation of vector x to vector b \n\n--- 第 42 页 ---\nIdentity and Inverse Matrices •Matrix inversion is a powerful tool to analytically solve Ax=b •Needs concept of Identity matrix •Identity matrix does not change value of vector •when we multiply the vector by identity matrix •Denote identity matrix that preserves n-dimensional vectors as In •Formally In ∈ Rn×n and ∀x ∈Rn , In x = x •Example of I3\n\n--- 第 43 页 ---\nMatrix Inverse •Inverse of square matrix A defined as A−1A=In •We can now solve Ax=b as follows:                      Ax=b                     A−1Ax = A−1b                      In x = A−1b                     x = A−1b •This depends on being able to find A-1 •If A-1 exists there are several methods for finding it \n--- 第 44 页 ---\nSolving Simultaneous equations •Ax = b•Two closed-form solutions •Matrix inversion x=A-1b •Gaussian elimination \n--- 第 45 页 ---\nNorms •Used for measuring the size of a vector •Norms map vectors to non-negative values •Norm of vector x is distance from origin to x •It is any function f that satisfies: \n\n--- 第 46 页 ---\nImage distance-=\n-=\n--- 第 47 页 ---\nLP Norm •Definition \n\n--- 第 48 页 ---\nLP Norm •Definition •L2 Norm•Called Euclidean norm, written simply as•Squared Euclidean norm is same as \n\n--- 第 49 页 ---\nLP Norm •Definition •L1 Norm•also called Manhattan distance \n\n--- 第 50 页 ---\nLP Norm •Definition •          Norm•also called max norm\n\n--- 第 51 页 ---\nNorms of two-dimensional Point\n00.511.522.533.544.55\n00.511.522.533.5Y-Values||x||1  =||x||2  =||x||⚯ =3+4 = 7\nX = (3,4)\n\n--- 第 52 页 ---\nImage distance-=\n-=L1 distance between X and Y:  L2 distance between X and Y:  \ndistance between X and Y:  \n\n--- 第 53 页 ---\nIntroduction to Scientific Python \n--- 第 54 页 ---\nNumpy•Fundamental package for scientific computing with Python •N-dimensional array object •Linear algebra, Fourier transform, random number capabilities •Building block for other packages (e.g. Scipy) •Open source\n--- 第 55 页 ---\nimport numpy as np •Basics: \n•Slicing as usual\n\n--- 第 56 页 ---\nMore basics numpy.arange:evenly spaced values within a given interval.\nnumpy.linspace:evenly spaced numbers over a specified interval.\n--- 第 57 页 ---\nMore basics \n•numpy.random.normal:•Draw random samples from a normal (Gaussian) distribution•loc: mean•scale: standard deviation\n--- 第 58 页 ---\nArrays are mutable \n\n--- 第 59 页 ---\nArray attributes\n•numpy.reshape:•Gives a new shape to an array without changing its data.\n--- 第 60 页 ---\nBasic operations •Arithmetic operators: elementwise application \n•Also, we can use += and *=. \n\n--- 第 61 页 ---\nArray broadcasting •When operating on two arrays, numpy compares shapes. Two dimensions are compatible when•They are of equal size•One of them is 1\n--- 第 62 页 ---\nArray broadcasting \n\n--- 第 63 页 ---\nArray broadcasting with scalars •This also allows us to add a constant to a matrix or multiply a matrix by a constant\n\n--- 第 64 页 ---\nVector operations •inner product •outer product •dot product (matrix multiplication)\n\n--- 第 65 页 ---\nMatrix operations •First, define some matrices: \n\n--- 第 66 页 ---\nMatrix operations •np.dot(a,b)•If f bothaandbare 1-D arrays, it is inner product of vectors•If bothaandbare 2-D arrays, it is matrix multiplication\n\n--- 第 67 页 ---\nOperations along axes \n\n--- 第 68 页 ---\nSlicing arrays More advanced slicing \n\n--- 第 69 页 ---\nIterating over arrays •Iterating over multidimensional arrays is done with respect to the first axis: for row in A •Looping over all elements: for element in A.flat \n--- 第 70 页 ---\nReshaping•Reshape •using reshape. Total size must remain the same. •Resize •using resize, always works: chopping or appending zeros •First dimension has ‘priority’, so beware of unexpected results\n--- 第 71 页 ---\nLinear algebra \n\n--- 第 72 页 ---\nRandom sampling \n\n--- 第 73 页 ---\nDistributions in random \n"}