{"status": "success", "type": "pdf", "task_id": "0a527ba5-6cac-43fa-bdf2-e96308b81fe8", "file_name": "0a527ba5-6cac-43fa-bdf2-e96308b81fe8_3c23ef0b5dc80b4dbdf34bc2d779396a.pdf", "content": "\n--- 第 1 页 ---\nLecture 5 -- Safety and Reliability IssuesProf. Xiaowei Huanghttps://cgi.csc.liv.ac.uk/~xiaowei/ (Attendance Code: 369434)\n--- 第 2 页 ---\n\n--- 第 3 页 ---\nGlobal Responsible AI Market Size\n3\nnot only is the solutionimportant, but also the services\n--- 第 4 页 ---\nMarket Trending for Trustworthy AI\n4•In 2021, the global AI economy was valued at US$59.7 billion, and the figure isforecast to reachUS$422 billion by 2028.•75 percentof all businesses already include AI in their core strategies. •80 percent of firms will commit at least10 percentof their AI budgets to regulatory compliance by 2024, with 45 percent pledging to set aside a minimum of 20 percent•90 percent of limited partners would walk away from an investment opportunity if it presented an ESG (Environmental, Social, and Corporate Governance) concern, to ensure that design and deploy AI that generates value without inflicting harm.•Globally, only 10 to 15% of companies have successfully industrialized AI-based solutions in their business and 30 to 40% are limited to experimentation.•A trusted AI market estimated at 52 billion euros in several sectors studied (Automotive, Railway, Aeronautics, Energy & Resources, Banking, Insurance, Pharmaceuticals).•A growing need among manufacturers for trust solutions paving the way for a market of certification or validation of AI systemshttps://www.accenture.com/us-en/insights/artificial-intelligence/ai-compliance-competitive-advantagehttps://ilpa.org/wp-content/uploads/2022/01/infographic-esg-measurement-gap-private-equity.pdfhttps://www.ey.com/fr_fr/strategy/quel-avenir-pour-l-intelligence-artificielle-dans-l-industrieIn summary, the current market size of products and service to enforce Verified and Trustworthy AI is between 5 to 10 billion euros.\n--- 第 5 页 ---\nTopics -- Safety Concerns  Generalisation error Adversarial examples Data poisoning Backdoor attackModel StealingMembership InferenceOthers (fairness, uncertainty, energy efficiency, etc)\n--- 第 6 页 ---\nSafety Concerns•Despite the success of machine learning in many areas, serious concerns have been raised in applying machine learning to real-world safety-critical systems such as self-driving cars, automatic medical diagnosis, etc.•Simply speaking, a learned model    is to approximate a target function   . Therefore, theerroneous behaviour ofexists when it is inconsistent with. \nf/hcatdog0.990.01\n--- 第 7 页 ---\nSafety Concerns\nf/hcatdog0.990.01n is the input dimension, e.g., n = c * l * h, where c is the number of channels, and l and h are the length and height of an image, respectively. k can be the number of classes. Prediction probability of x on label jPrediction class\n--- 第 8 页 ---\nGeneralisation Error•Empirical loss:•Expected loss:•Generalisation error: \n: Loss between prediction f(x) and ground truth yD is the set of training dataset, and       is the underlying data distribution\n\n--- 第 9 页 ---\nGeneralisation Error\nGeneralisation error is closely related to the overfitting problem of machine learning algorithms.\nA machine learning model is overfitted if it performs well on training data samples but badly on test data samples\n--- 第 10 页 ---\nAdversarial Examples•Adversarial examples represent another class of erroneous behaviours that also introduce safety implications. •It represents a mis-match of the decisions made by a human and by a neural network, and does not necessarily involve an adversary.\n\n--- 第 11 页 ---\nAdversarial Examples\n•given an instance x, it is classified correctly, i.e., •but a small perturbation on x may lead to a change of classification, i.e., \n\n--- 第 12 页 ---\nAdversarial Examples\n\n--- 第 13 页 ---\nMeasurement of Adversarial Examples•An adversarial example is usually measured from two aspects:•magnitude of perturbation,     where             is a norm distance, •probability gap between and after the perturbation, i.e., \n\n--- 第 14 页 ---\nOptimisation problem for adv. examples \nminimisemaximise\nUnder these constraints\n--- 第 15 页 ---\nData Poisoning\n•Poisoning attack occurs when the adversary injects malicious data into training process, and hence get a machine learning algorithm to learn something it should not. •There are two types of poisoning attacks, •one for data poisoning attacks and •the other for backdoor attacks. \n--- 第 16 页 ---\nBackdoor attack\n•Given a triggered input x’ = x + ⍺, where ⍺ is the trigger stamped on a “clean” input x, the predicted label will always be the label y⍺ that is set by the attacker, regardless of what the input x might be. •as long as the triggered input x’ is present, the backdoor model will always classify the input to the attacker's target label (i.e., y⍺). •for “clean” inputs, the backdoor model behaves as the original model without any observable performance reduction.\n--- 第 17 页 ---\nBackdoor attack\n\n--- 第 18 页 ---\nModel Stealing•Given a model f, a model stealing agent is to reconstruct another model f’ by e.g., querying the model f.\n\n--- 第 19 页 ---\nMembership inference•Membership inference is to identify the training data for a trained model.•Formally, it is, given a data instance x and the access to a model f, to determine if the instance x was in the model’s training dataset, i.e., if x \\in Dtrain.\n\n--- 第 20 页 ---\nOther safety and reliability concerns \n•Privacy •Fairness •Energy Efficiency•Uncertainty•etc \n--- 第 21 页 ---\nAI Safety•Technical Issues•Robustness (example)•Uncertainty (example)•Security, privacy, fairness, XAI, etc•Socio-technical issues •Alignment •User Trust•Governance issues\n--- 第 22 页 ---\n\n--- 第 23 页 ---\nRobustness in Autonomous Driving\n\n--- 第 24 页 ---\nUncertainty in Autonomous Driving\n\n--- 第 25 页 ---\nFoundation Models\nImages from internet, subject to copyright restriction\n--- 第 26 页 ---\nExample Application in Autonomous Driving\n\n--- 第 27 页 ---\nGenAI Safety•Technical Issues•Robustness (example)•Uncertainty (multiple queries on the same input lead to different outputs)•Hallucinations (example)•Deepfake (misinformation, etc)•Harm (example)•Socio-technical issues •Alignment •User Trust•Data copyright •Open source •Governance issues\n--- 第 28 页 ---\nApproaches to deal with GenAI Safety\n\n--- 第 29 页 ---\nRobustness of Large Language Models\n\n--- 第 30 页 ---\nHarmfulness (Illustration)\n\n--- 第 31 页 ---\nHallucinations•Responses that are either nonexistent in reality, illogical, or irrelevant to the prompt provided\n•Generate answers based on patterns identified in training datasets, rather than actual factual understanding•traced back to various stages including data sourcing, pre-training, alignment, and inference\n\n--- 第 32 页 ---\nConclusions•Models are bigger, more and more black-box•Data becomes inaccessible•Open source becomes a discussion issue•More intelligence will potentially lead to emergent issues •Protection will be more on runtime, e.g., guardrails•Human centricity becomes indispensable •When will scaling law become inapplicable? "}