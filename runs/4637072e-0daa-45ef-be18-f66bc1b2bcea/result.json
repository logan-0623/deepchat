{"status": "success", "type": "pdf", "task_id": "4637072e-0daa-45ef-be18-f66bc1b2bcea", "file_name": "4637072e-0daa-45ef-be18-f66bc1b2bcea_7860f902b7c5788d28c43e087aab2614.pdf", "content": "\n--- 第 1 页 ---\nLecture 13  -- Naïve BayesProf Xiaowei Huanghttps://cgi.csc.liv.ac.uk/~xiaowei/ (Attendance Code: 466202)\n\n--- 第 2 页 ---\nUp to now,•Three machine learning algorithms: •decision tree learning •k-nn •linear regression + gradient descent \nAlso useful for un/semi-supervised learningLearn how to design new objective function and how to adapt optimisation algorithm for new objective functionA good connection between symbolic AI and machine learning\n--- 第 3 页 ---\nTopics•Naïve Bayes •Theoretical Analysis on Computational Efficiency via Parameter Estimation •Naïve Bayes Algorithm\n1.Preparation for graphical models;2.Learn how assumptions can simply a complex problem and lead to useful conclusionsBayesian learning is a key branch of machine learning\n--- 第 4 页 ---\nNaïveBayes Algorithm•Given data for X and Y , such that for each data instance (x,y), xhas ndimensions and yis a label.  •Goal: classify (xnew) \nHow to calculate these two terms? \nTypical classification problemBayes RuleThe essence of Naïve Bayes is to have an efficientmethod for the computation of this term by having an assumption.This term can be easily estimated from the training dataset.\n--- 第 5 页 ---\nTheorical Analysis on Computational Efficiency via Estimation of Parameters\n--- 第 6 页 ---\nLet’s learn classifiers by learning P(Y|X) •Consider Y=Wealth, X=<Gender, HoursWorked> GenderHoursWorkedWealthProbabilityFemale<40.5Poor0.253122Rich0.0245895>40.5Poor0.0421768Rich0.0116293Male<40.5Poor0.331313Rich0.0971295>40.5Poor0.134106Rich0.105933P(gender, hoursWorked, wealth) X1X2Y\nhow many parameters need to estimate? 7Can we estimate P(Y)? If so, how? \n--- 第 7 页 ---\nLet’s learn classifiers by learning P(Y|X) \n•P(gender, hoursWorked, wealth) => P(wealth|gender, hoursWorked) P(Y|X)\nHow many parameters to estimate? 4How about estimating P(X|Y)? \n--- 第 8 页 ---\nHow many parameters must we estimate? •Suppose X =<X1,... Xn> where Xiand Y are Boolean real variables•To estimate P(Y|X1, X2, ... Xn), how many quantitates need to be estimated or collected?  2n•If we have 30 Boolean Xi’s: P(Y | X1, X2, ... X30) 230~ 1 billion! •You need lots of data or a very small n \nn=2, and we need to estimate 4 quantities. Why 4?  These 4 are neededThese 4 can be obtained by 1-x Idea: Shall we now consider P(X|Y)?\n--- 第 9 页 ---\nCan we reduce parameters using Bayes Rule? •Suppose X =<X1,... Xn> where Xiand Y are Boolean real variables •By Bayes rule: •How many parameters for P(X|Y) = P(X1,... Xn|Y)? (2n-1)x2 •How many parameters for P(Y)? 1\nFor example, P(Gender,HrsWorked|Wealth)For example, P(Wealth)GenderHrsWorkedP(G,HW|rich)P(G,HW|poor)F<40.50.10280.3327F>40.50.04860.0554M<40.50.40590.4355M>40.50.44270.1763Y(X|Y)\nP(poor)P(rich)0.76071870.2392813P(Y)So, by now, we haven’t achieved the reduction of parameters. P(Y|X) needs 2n, while P(X|Y)P(Y) needs (2n-1)x2+1=2n+1-1. Any further idea?\n--- 第 10 页 ---\nNaïveBayes Assump?on •NaïveBayes assumesi.e., that Xiand Xjare condi7onally independent given Y , for all i≠j \nFor example, P(Gender,HrsWorked|Wealth) = P(Gender|Wealth) * P(HrsWorked|Wealth)\n--- 第 11 页 ---\nRecap: Conditional independence •Two variables A,B are independentif •Two variables A,B are conditionally independentgiven C if •We also have \nTwo equivalent definitions.\n--- 第 12 页 ---\nEffect of Naïve Bayes Assumption•NaïveBayes uses assumpDon that the Xiare condiDonally independent, given Y •Given this assumpDon, then: •in general: \nChain rule Deﬁnihon of Condihonal Independence (2n-1)x2 2nWhy? Every P(Xi|Y) takes a parameter, and we have n Xi. \n--- 第 13 页 ---\nSummary of Naïve Bayes Algorithm •To make this tractable we naively assume conditional independence of the features given the class: ie•Now: I only need to estimate ... parameters: •Besides, we also need to estimate P(Y)\nStep 1Step 2\n--- 第 14 页 ---\nSummary of the Beneﬁt of Naïve Bayes AlgorithmHow many parameters to describe                             ?            ?•Without conditional independent assumption? (2n-1)x2+1 •With conditional independent assumption? 2n+1\nWhich is opposed to 2n before considering naïve bayes.  \n--- 第 15 页 ---\nNaïveBayes Algorithm --A 3-stepprocedure \n--- 第 16 页 ---\nNaïve Bayes Algorithm –discrete Xi•Train NaïveBayes (given data for X and Y) •For each value•Es#mate•For each value   of each aDribute •es#mate \nStep 2Step 1\n--- 第 17 页 ---\nTraining Naïve Bayes Classiﬁer•From the data D, estimate class priors:•For each possible value of Y , estimate Pr(Y=y1), Pr(Y=y2),.... Pr(Y=yk) •An estimate: •From the data, estimate the conditional probabilities •If every Xihas values xi1,...,xik•for each yiand each Xiestimate q(i,j,k)=Pr(Xi=xij|Y=yk) •\nNumber of items in dataset D for which Y=ykStep 2Step 1Number of items in dataset D for which Xi=xijand Y=yk\n--- 第 18 页 ---\nExercise\n•Consider the following dataset: •P(Wealthy=Y) = •P(Wealthy=N)=•P(Gender=F | Wealthy = Y) =•P(Gender=M | Wealthy = Y) =  •P(HrsWorked> 40.5 | Wealthy = Y) =•P(HrsWorked< 40.5 | Wealthy = Y) =  •P(Gender=F | Wealthy = N) =•P(Gender=M | Wealthy = N) =  •P(HrsWorked> 40.5 | Wealthy = N) =•P(HrsWorked< 40.5 | Wealthy = N) =  GenderHrsWorkedWealthy?F39YF45NM35NM43NF32YF47YM34YCheck the exercises of lecture notes for answers\n--- 第 19 页 ---\nNaïve Bayes Algorithm –discrete Xi•Train NaïveBayes (given data for X and Y) •for each value•EsGmate•for each value   of each aIribute •esGmate •Classify (Xnew) \nStep 3Step 1Step 2\n--- 第 20 页 ---\nExercise (Continued)\n•Consider the following dataset: •Classify a new instance •Gender = F /\\HrsWorked= 44GenderHrsWorkedWealthy?F39YF45NM35NM43NF32YF47YM34YCheck the exercises of lecture notes for answers\n--- 第 21 页 ---\nExample: Live outside of Liverpool? P(L|T,D,E) •L=1 iﬀlive outside of Liverpool • D=1 iﬀDrive or Carpool to Liverpool •T=1 iﬀshop at Tesco                   • E=1 iﬀEven # leTers last name P(L=1) : P(D=1 | L=1) : P(D=1 | L=0) : P(T=1 | L=1) : P(T=1 | L=0) : P(E=1 | L=1) : P(E=1 | L=0) : P(L=0) : P(D=0 | L=1) : P(D=0 | L=0) : P(T=0 | L=1) : P(T=0 | L=0) : P(E=0 | L=1) : P(E=0 | L=0) : "}