{"status": "success", "type": "pdf", "task_id": "a240cdc2-6a3d-42bc-b844-b1fa7d861d1a", "file_name": "a240cdc2-6a3d-42bc-b844-b1fa7d861d1a_0071773ea04a8fdb61ecf4cab4c7cde5.pdf", "content": "\n--- ç¬¬ 1 é¡µ ---\nLecture 10 -- K-Nearest NeighbourProf. Xiaowei Huanghttps://cgi.csc.liv.ac.uk/~xiaowei/(Attendance Code:  406355)\n\n--- ç¬¬ 2 é¡µ ---\nUp to now, \nMachine Learning Basic Knowledge \nSafety and Reliability Properties (but the algorithms on how to attack and defend will be introduced later)\nDecision Tree Learning\n--- ç¬¬ 3 é¡µ ---\nConfidence for decision tree (example)\nâ€¢Random forest:â€¢Multiple decision trees are trained, by using different resamples of your data.â€¢Probabilities can be calculated by the proportion of decision trees which vote for each class. â€¢For example, if 8 out of 10 decision trees vote to classify an instance as positive, we say that the confidence is 8/10. Here, the confidences of all classes add up to 1\n--- ç¬¬ 4 é¡µ ---\nToday â€™s Topics â€“ K-nearest neighbor\nBasic AlgorithmHow can we determine similarity/distanceStandardizing numeric features (leave this to you) \nSpeeding up k-NNEdited nearest neighbour k-d trees for nearest neighbour identification\nVariants of k-NN K-NN regressionDistance-weighted nearest neighbor Locally weighted regression to handle irrelevant features\nDiscussionsStrengths and limitation of instance-based learning, which is a predication based on the similarity of the query to its nearest neighbor(s))Algorithmic content\n--- ç¬¬ 5 é¡µ ---\nK-NN algorithm, illustrated\n\n--- ç¬¬ 6 é¡µ ---\nNearest-neighbor classification \nâ€¢Learningstageâ€¢ Given a training set (x(1), y(1)) ... (x(m) , y(m)), do nothingâ€¢(itâ€™s sometimes called a lazylearner) â€¢Classificationstageâ€¢given: an instance x(q)to classifyâ€¢find the training-set instance x(i)that is most similar to x(q)â€¢return the class value y(i) \n--- ç¬¬ 7 é¡µ ---\nThe decision regions for nearest-neighbor classification â€¢Voronoi diagram: each polyhedron indicates the region of feature space that is in the nearest neighborhood of each training instance \nFor the case when k=1 and we have two features X1and X2\n--- ç¬¬ 8 é¡µ ---\nNearest Neighbor â€¢Less than 20 attributes per instance â€¢Lots of training data When to Consider â€¢Training is very fastâ€¢Learn complex target functions â€¢Do not lose information Advantages \nâ€¢Slow at query timeâ€¢Easily fooled by irrelevant attributes Disadvantages \n--- ç¬¬ 9 é¡µ ---\nk-nearest-neighborclassification: Algorithm \nâ€¢classification taskâ€¢given: an instance x(q)to classify â€¢find the ktraining-set instances (x(1), y(1))... (x(k), y(k)) that are the most similar to x(q)â€¢return the class value â€¢(i.e. return the class that has the most number of instancesin the k training instances)\n\n--- ç¬¬ 10 é¡µ ---\nExampleâ€¢Suppose all features are categorical  â€¢Hamming distance (or L0norm): count the number of features for which two instances differ â€¢Example: X = (Weekday, Happy?, Weather)  Y = AttendLecture? â€¢D : in the tableâ€¢New instance: <Friday, No, Rain>â€¢Distances = {2, 3, 1, 2} â€¢For 1-nn, which instances should be selected? â€¢For 2-nn, which instances should be selected? â€¢For 3-nn, which instances should be selected? v1v2v3yWedYesRainNoWedYesSunnyYesThuNoRainYesFriYesSunnyNoFriNoRainNew datum\n--- ç¬¬ 11 é¡µ ---\nExampleâ€¢Example: X = (Weekday, Happy?, Weather)  Y = AttendLecture? â€¢New instance: <Friday, No, Rain>â€¢For 3-nn, selected instances: {(<Wed, Yes, Rain>, No), (<Thu, No, Rain>, Yes), (<Fri, Yes, Sunny>, No)}â€¢Classification:â€¢v = Yes.â€¢v = No. \nSo, which class this new instance should be in? kXi=1\u0000(v, y(i))=1+0+1=2\n<latexit sha1_base64=\"T96nE93CkXvny6X6vmsrOwBJI7c=\">AAACDnicbVDLSsNAFJ34rPUVdekmWAotlZJUQTeFohuXFewDmjRMJpN26OTBzKQQQr7Ajb/ixoUibl2782+ctllo64ELh3Pu5d57nIgSLnT9W1lb39jc2i7sFHf39g8O1aPjLg9jhnAHhTRkfQdyTEmAO4IIivsRw9B3KO45k9uZ35tixkkYPIgkwpYPRwHxCIJCSrZaNnns2ylpGtlwYrqYCliZnifDtEKqWbVp1PSa0WzYakmv63Noq8TISQnkaNvql+mGKPZxIBCFnA8MPRJWCpkgiOKsaMYcRxBN4AgPJA2gj7mVzt/JtLJUXM0LmaxAaHP190QKfc4T35GdPhRjvuzNxP+8QSy8ayslQRQLHKDFIi+mmgi1WTaaSxhGgiaSQMSIvFVDY8ggEjLBogzBWH55lXQbdeOi3ri/LLVu8jgK4BScgQowwBVogTvQBh2AwCN4Bq/gTXlSXpR35WPRuqbkMyfgD5TPHzUymj8=</latexit>\n\n--- ç¬¬ 12 é¡µ ---\nHow can we determine similarity/distance â€¢Suppose all features are continuous, where xf(i)represents the f -thfeature of x(i)â€¢Euclidean distance: â€¢Manhattan distance: \nRecall the difference and similarity with Lp norm\n--- ç¬¬ 13 é¡µ ---\nMore exampleâ€¢Example: X = (Height, Weight, RunningSpeed)  Y = SoccerPlayer? â€¢D: in the tableâ€¢New instance: <185, 91, 13.0>â€¢Suppose that Euclidean distance is used. â€¢Is this person a soccer player? v1v2v3y1828711.3No1899212.3Yes1787910.6Yes1839012.7No1859113.0New datumLeave this as exercise for you. \n--- ç¬¬ 14 é¡µ ---\nHow to work with a mix of discrete/continuous featuresâ€¢If we have a mix of discrete/continuous features: â€¢Typically want to apply to continuous features some type of normalization (values range 0 to 1) or standardization (values distributed according to standard normal) â€¢Many other possible distance functions we could use ... \n\n--- ç¬¬ 15 é¡µ ---\nPre-processing â€“Standardisation of numeric features â€¢Given the training set D, determine the mean and stddevfor feature xiâ€¢Standardize each value of feature xias follows â€¢Do the same for test instances, using the same ðœ‡and ðœŽderived from the training data \n\n--- ç¬¬ 16 é¡µ ---\nConfidence for k-NN classification (example)â€¢Classification steps are the same, recallâ€¢Given a class    , we computeâ€¢apply sigmoid function on the reciprocal of the accumulated distance  \nAccumulated distance to the supportive instancesHere, the confidences of all classes may not add up to 1Softmax?\n--- ç¬¬ 17 é¡µ ---\nSpeeding up k-NN\n--- ç¬¬ 18 é¡µ ---\nIssuesâ€¢Choosing kâ€¢Increasing k reduces variance, increases bias â€¢For high-dimensional space, problem that the nearest neighbor may not be very close at all! â€¢Memory-based technique. Must make a pass through the data for each classification. This can be prohibitive for large data sets. \n--- ç¬¬ 19 é¡µ ---\nNearest neighbour problem â€¢Given sample S = ((x1,y1),...,(xm,ym)) and a test point x, â€¢It is to find the nearest k neighbours of x. â€¢Note: for the algorithms, dimensionality N, i.e., number of features, is crucial. \n\n--- ç¬¬ 20 é¡µ ---\nEfficient Indexing: N=2â€¢Algorithm â€¢compute Voronoi diagram in O(m log m)â€¢See algorithm in https://en.wikipedia.org/wiki/Fortune's_algorithmâ€¢use point location data structure to determine nearest neighbours  â€¢complexity: O(m) space, O(log m) time. \n\n--- ç¬¬ 21 é¡µ ---\nEfficient Indexing: N>2â€¢Voronoi diagram: size in O(mN/2) â€¢Linear algorithm (no pre-processing): â€¢compute distance ||x âˆ’ xi|| for all iâˆˆ[1, m]. â€¢complexity of distance computation: Î©(N m). â€¢no additional space needed. k-NN is a â€œlazyâ€ learning algorithm â€“does virtually nothing at training time but classification/prediction time can be costly when the training set is large \n--- ç¬¬ 22 é¡µ ---\nEfficient Indexing: N>2\nâ€¢Two general strategies for alleviating this weakness â€¢donâ€™t retain every training instance (edited nearest neighbor) â€¢pre-processing. Use a smart data structure to look up nearest neighbors(e.g. a k-d tree) \n--- ç¬¬ 23 é¡µ ---\nEdited instance-based learning â€¢Select a subset of the instances that still provide accurate classifications â€¢Incremental deletionâ€¢Incremental growth\nQ1: Does ordering matter? Q2: If following the optimal ordering, do the two approaches produce the same subset of instances?  \n--- ç¬¬ 24 é¡µ ---\nk-d trees â€¢A k-d tree is similar to a decision tree except that each internal node â€¢stores one instance â€¢splits on the median value of the feature having the highest variance \n\n--- ç¬¬ 25 é¡µ ---\nConstruction of k-d tree\nDetermine a triple: nthe feature having the highest variancenMedian value of the feature nRelated data point. Example:  --point f, x1= 6x1>6             f\n--- ç¬¬ 26 é¡µ ---\nConstruction of k-d tree\nDetermine a triple: nthe feature having the highest variancenMedian value of the feature nRelated data point. x1>6             fx2>10             cx2>5             hExample:  --point f, x1= 6--point c, x2= 10and point h, x2 = 5\n--- ç¬¬ 27 é¡µ ---\nConstruction of k-d tree\nThere can be other methods of constructing k-d trees, see e.g., https://en.wikipedia.org/wiki/K-d_tree#Nearest_neighbour_search\n--- ç¬¬ 28 é¡µ ---\nFinding nearest neighborson a k-d tree â€¢use branch-and-bound search â€¢priority queue stores â€¢nodes consideredâ€¢lower bound on their distance to query instance â€¢lower bound given by distance using a single feature â€¢average case: O(log2m) â€¢worst case: O(m) where m is the size of the training-set \n--- ç¬¬ 29 é¡µ ---\nFinding nearest neighbours in a k-d tree \nIntuitively, for a pair (node,bound), bound represents the smallest guaranteed distance, i.e., greatest lower bound up to now, from the instance x(q) to the set of instances over which node is the selected one to splitFor example, the set of instances where root is the selected one to split over is the whole training set. (root,0) means that at the beginning, the guaranteed smallest distance to the training set is 0General idea: maintain a best_distand a priority queue of (node,bound)pair, and terminate when best_distbecomes no greater than bound.\n--- ç¬¬ 30 é¡µ ---\nk-d tree example (Manhattan distance) \nCanâ€™t see any moreAvailable information\n--- ç¬¬ 31 é¡µ ---\nk-d tree example (Manhattan distance) \nAvailable information\nInitialise best_distas infinityPush in the root node\n--- ç¬¬ 32 é¡µ ---\nk-d tree example (Manhattan distance) \nAvailable information\nThe distance between qand f\n--- ç¬¬ 33 é¡µ ---\nk-d tree example (Manhattan distance) \nAvailable information\nWhy 0?Because qand care on the same side of f\n--- ç¬¬ 34 é¡µ ---\nk-d tree example (Manhattan distance) \nAvailable information\nWhy 4?Because the shortest distance from qto the other side of fis 44\n--- ç¬¬ 35 é¡µ ---\nk-d tree example (Manhattan distance) \nAvailable information\nThe distance between qand c\n--- ç¬¬ 36 é¡µ ---\nk-d tree example (Manhattan distance) \nAvailable information\neand qare at the same side of cband qare at the different side of c, and 7 is the shortest distance from qto the boundary\n--- ç¬¬ 37 é¡µ ---\nk-d tree example (Manhattan distance) \nAvailable information\nThe distance between qand eNow we can update best_distbecause 1<4\n--- ç¬¬ 38 é¡µ ---\nk-d tree example (Manhattan distance) \nAvailable information\ndand qare at the different side of e, and 1 is the shortest distance from qto the boundary\n--- ç¬¬ 39 é¡µ ---\nk-d tree example (Manhattan distance) \nAvailable information\nNow we have best_dist= bound, return the node e\n--- ç¬¬ 40 é¡µ ---\nExtended Materials: Voronoi Diagram Generationâ€¢https://en.wikipedia.org/wiki/Voronoi_diagramâ€¢https://courses.cs.washington.edu/courses/cse326/00wi/projects/voronoi.html\n\n--- ç¬¬ 41 é¡µ ---\nVariants of k-NN \n\n--- ç¬¬ 42 é¡µ ---\nk-nearest-neighbor regression â€¢learning stageâ€¢ given a training set (x(1), y(1)) ... (x(m) , y(m)), do nothing â€¢(itâ€™s sometimes called a lazy learner) â€¢classification stageâ€¢given: an instance x(q)to classifyâ€¢find the k training-set instances (x(1), y(1))... (x(k), y(k)) that are most similar to x(q)â€¢return the value\nAverage over neighboursâ€™ values\n--- ç¬¬ 43 é¡µ ---\nDistance-weighted nearest neighbor â€¢We can have instances contribute to a prediction according to their distance from x(q)â€¢classification: â€¢regression: \nreciprocal of the distanceIntuition: instances closer to the current one is more important. \n--- ç¬¬ 44 é¡µ ---\nIrrelevant features in instance-based learning hereâ€™s a case in which thereis one relevant feature x1and a 1-NN rule classifies each instance correctly consider the effect of an irrelevant feature x2on distances and nearest neighbors\nCan you find a point (a,b) which is red, if classified only according to feature x1, and is green, if classified according to both features? \n--- ç¬¬ 45 é¡µ ---\nLocally weighted regression â€¢one way around this limitation is to weight features differently â€¢locally weighted regression is one nearest-neighborvariant that does this â€¢prediction task â€¢given: an instance x(q) to make a prediction for â€¢find the k training-set instances (x(1), y(1)) ... (x(k), y(k)) that are most similar to x(q)â€¢return the value \nWhatâ€™s function f ?\n--- ç¬¬ 46 é¡µ ---\nLocally weighted regression â€¢Determining function fâ€¢Assume that f is a linear function over the features, i.e., â€¢find the weights wifor each x(q)byâ€¢After obtaining weights, for x(q), we have \ncan do this using gradient descent (to be introduced soon) \n\n--- ç¬¬ 47 é¡µ ---\nDiscussions\n--- ç¬¬ 48 é¡µ ---\nStrengths of instance-based learning \nâ€¢simple to implement â€¢â€œtrainingâ€ is very efficient â€¢adapts well to on-line learning â€¢robust to noisy training data (when k > 1) â€¢often works well in practice \n--- ç¬¬ 49 é¡µ ---\nLimitations of instance-based learning \nâ€¢sensitive to range of feature values â€¢sensitive to irrelevant and correlated features, although ... â€¢there are variants (such as locally weighted regression) that learn weights for different features â€¢classification/prediction can be inefficient, although â€¦â€¢edited methods and k-d trees can help alleviate this weakness â€¢doesnâ€™t provide much insight into problem domain because there is no explicit model "}