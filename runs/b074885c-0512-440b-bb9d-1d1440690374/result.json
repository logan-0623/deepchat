{"status": "success", "type": "pdf", "task_id": "b074885c-0512-440b-bb9d-1d1440690374", "file_name": "b074885c-0512-440b-bb9d-1d1440690374_2c9437b98ecea5c6c0f32bc166dcd363.pdf", "content": "\n--- Á¨¨ 1 È°µ ---\nLecture 11 --Linear and Logistic RegressionProf. Xiaowei Huanghttps://cgi.csc.liv.ac.uk/~xiaowei/ (Attendance Code: 825019)\n\n--- Á¨¨ 2 È°µ ---\nUp to now,‚Ä¢Two Classical Machine Learning Algorithms ‚Ä¢Decision tree learning‚Ä¢K-nearest neighbor‚Ä¢What is k-nearest-neighbor classification ‚Ä¢How can we determine similarity/distance‚Ä¢Standardizing numeric features‚Ä¢Speeding up k-NN‚Ä¢edited nearest neighbour ‚Ä¢k-d trees for nearest neighbour identification\n\n--- Á¨¨ 3 È°µ ---\nToday ‚Äôs TopicsIndictive Bias\nlinear regression mean square error\nlinear classification 0-1 loss\nlogistic regression SigmoidProbability as confidenceLog functionAn example to learn how to design objective function.\n--- Á¨¨ 4 È°µ ---\nInductive bias ‚Ä¢inductive bias is the set of assumptions a learner uses to be able to predict yifor a previously unseen instance xi‚Ä¢two components ‚Ä¢hypothesis space bias: determines the models that can be represented ‚Ä¢preference bias: specifies a preference ordering within the space of models ‚Ä¢in order to generalize (i.e. make predictions for previously unseen instances) a learning algorithm must have an inductive bias \n\n--- Á¨¨ 5 È°µ ---\nConsider the inductive bias of DT and k-NN learners \n\n--- Á¨¨ 6 È°µ ---\nLinear regression \n\n--- Á¨¨ 7 È°µ ---\nRecap: dot product in linear algebra\nGeometric meaning: can be used to understand the angle between two vectorsDot product is a measure of how closely two vectors align, in terms of the directions they point.\n--- Á¨¨ 8 È°µ ---\nLinear regression, illustrated ‚Ä¢Given training data                                               i.i.d.from distribution ùê∑\nUse curves of a specific shape to fit the data\n--- Á¨¨ 9 È°µ ---\nLinear regression, illustrated ‚Ä¢Given training data                                               i.i.d. from distribution ùê∑‚Ä¢Find                           that minimises \n\n--- Á¨¨ 10 È°µ ---\nDefinition of linear regression  ‚Ä¢Given training data                                               i.i.d.from distribution ùê∑‚Ä¢Find                           that minimises \nHypothesis Class HL2 loss, or mean square error\n--- Á¨¨ 11 È°µ ---\nDefinition of linear regression ‚Ä¢Given training data                                               i.i.d.from distribution ùê∑‚Ä¢Find                           that minimises where‚Ä¢‚Ä¢\nrepresents the mean square error of all training instances\nSo,represents the error of instancerepresents the squareerror of alltraining instances\n--- Á¨¨ 12 È°µ ---\nDefinition of linear regression‚Ä¢Given training data                                               i.i.d.from distribution ùê∑‚Ä¢Find                          that minimises ‚Ä¢Let ùëãbe a matrix whose ùëñ-throw is               , ùë¶be the vector \nWe will use example to explain this!Solving this optimization problem will be introduced in later lectures.\n--- Á¨¨ 13 È°µ ---\nExample‚Ä¢Let ùëãbe a matrix whose ùëñ-throw is               \nFootball player example: (height, weight, runningspeed)Question: how to compute loss? Assume a weight vector \nNote: for linear regression, this is the parameter vector we want to learn. Here, we provide an example for illustration. Why care about loss? The smaller the loss, the better the weight vector. \n\n--- Á¨¨ 14 È°µ ---\nMethod 1: Compute loss individually‚Ä¢Assume a function                            with weight vector\nAnd then, computewhere \n\n--- Á¨¨ 15 È°µ ---\nMethod 2, Step 1: Organise feature data into matrix‚Ä¢Let ùëãbe a matrix whose ùëñ-throw is               \nv1v2v3y1828711.3325 (No)1899212.3344 (Yes)1787910.6350 (Yes)1839012.7320 (No)\nFootball player example: (height, weight, runningspeed)\n--- Á¨¨ 16 È°µ ---\nMethod 2, Step 2: Matrix Computation\nComputeThen, by \n<latexit sha1_base64=\"20yZHTIpaRe1Nnjv4ZBd5LyePwM=\">AAAB83icbVDLSsNAFL3xWeur6tLNYBHcWJLia1l047KCfUAby2Q6aYdOJmFmooSkv+HGhSJu/Rl3/o3TNgttPXDhcM693HuPF3GmtG1/W0vLK6tr64WN4ubW9s5uaW+/qcJYEtogIQ9l28OKciZoQzPNaTuSFAcepy1vdDPxW49UKhaKe51E1A3wQDCfEayN1M2y9tNpkmW96kO1VyrbFXsKtEicnJQhR71X+ur2QxIHVGjCsVIdx460m2KpGeF0XOzGikaYjPCAdgwVOKDKTac3j9GxUfrID6UpodFU/T2R4kCpJPBMZ4D1UM17E/E/rxNr/8pNmYhiTQWZLfJjjnSIJgGgPpOUaJ4Ygolk5lZEhlhiok1MRROCM//yImlWK85F5fzurFy7zuMowCEcwQk4cAk1uIU6NIBABM/wCm9WbL1Y79bHrHXJymcO4A+szx/Aq5GC</latexit>||Xw\u0000y||22, we compute\nWe can check that\n--- Á¨¨ 17 È°µ ---\nVariant: Linear regression with bias \n--- Á¨¨ 18 È°µ ---\nLinear regression with bias ‚Ä¢Given training data                                               i.i.d. from distribution ùê∑‚Ä¢Find                                       that minimises the loss‚Ä¢Reduce to the case without bias: ‚Ä¢Let ‚Ä¢Then \nBias TermIntuitively, every instance is extended with one more feature whose value is always 1, and we already know the weight for this feature, i.e., b\n--- Á¨¨ 19 È°µ ---\nLinear regression with bias ‚Ä¢Think about bias                          for the football player example \nCan do a bit of exercise on this. \n<latexit sha1_base64=\"/pWjxTJvlQJJjHrI2KWU6lNvOek=\">AAAChnicbVHRTtswFHUCYyVs0LFHXiyqse0lilMg7cOkir3wCNIKlZqqctzb1sJxIttBVFE/hZ/a2/4Gp40YFI6U+OTcc+Sbe5NccG2C4J/jbm1/2PnY2PX2Pn3eP2h+ObzRWaEY9FkmMjVIqAbBJfQNNwIGuQKaJgJuk7vfVf32HpTmmfxjFjmMUjqTfMoZNVYaNx8H33/FCcy4LJOUGsUflh62IJ0Qr3GCO5F9YUL8tj0JjuPa0X12dMOqEm4Yoo79jLqVFvjnr7Pt/9lgnY1WBi8GOXluZdxsBX6wAn5LSE1aqMbVuPk3nmSsSEEaJqjWQxLkZlRSZTgTsPTiQkNO2R2dwdBSSVPQo3I1xiX+ZpUJnmbKPtLglfoyUdJU60WaWKftb643a5X4Xm1YmGlnVHKZFwYkW180LQQ2Ga52gidcATNiYQllitteMZtTRZmxm/PsEMjmL78lN6FPzv2z69NW76IeRwMdoWP0AxEUoR66RFeoj5iz5fx0QqftNlzfPXOjtdV16sxX9Apu7wnDN6p0</latexit>X0=2664182 87 11.31189 92 12.31178 79 10.61183 90 12.713775<latexit sha1_base64=\"yD+WVX66k15MrY6+taGw1AgMPZE=\">AAAB+nicbVDLTsJAFJ3iC/FVdOlmIjG6ANKCr40J0Y1LTOSRlIZMhylMmE6bmamEVD7FjQuNceuXuPNvHKALBU9yk5Nz7s2993gRo1JZ1reRWVldW9/Ibua2tnd298z8flOGscCkgUMWiraHJGGUk4aiipF2JAgKPEZa3vB26rceiZA05A9qHBE3QH1OfYqR0lLXzI9Orh27WLKLFatYqlYtt2sWrLI1A1wmdkoKIEW9a351eiGOA8IVZkhKx7Yi5SZIKIoZmeQ6sSQRwkPUJ46mHAVEusns9Ak81koP+qHQxRWcqb8nEhRIOQ483RkgNZCL3lT8z3Ni5V+5CeVRrAjH80V+zKAK4TQH2KOCYMXGmiAsqL4V4gESCCudVk6HYC++vEyalbJ9UT6/PyvUbtI4suAQHIFTYINLUAN3oA4aAIMReAav4M14Ml6Md+Nj3pox0pkD8AfG5w/a1pEq</latexit>w0=[ 1,\u00001,20,\u0000330]Then, we haveFinally,\n--- Á¨¨ 20 È°µ ---\nVariant: Linear regression with lasso penalty \n--- Á¨¨ 21 È°µ ---\nLinear regression with lasso penalty ‚Ä¢Given training data                                               i.i.d. from distribution ùê∑‚Ä¢Find                                       that minimises the loss\nlasso penalty: L1norm of the parameter, encourages sparsity \n--- Á¨¨ 22 È°µ ---\nVariant: Evaluation Metrics \n--- Á¨¨ 23 È°µ ---\nEvaluation Metrics ‚Ä¢Root mean squared error (RMSE) ‚Ä¢Mean absolute error (MAE) ‚Äìaverage L1error ‚Ä¢R-square (R-squared) ‚Ä¢Historically all were computed on training data, and possibly adjusted after, but really should cross-validate \n--- Á¨¨ 24 È°µ ---\nLinear classification\n\n--- Á¨¨ 25 È°µ ---\nLinear classification: natural attempt \n\n--- Á¨¨ 26 È°µ ---\nLinear classification: natural attempt ‚Ä¢Given training data                                               i.i.d. from distribution ùê∑‚Ä¢Hypothesis ‚Ä¢ùë¶= 1 if ùë§ùëáùë•> 0 ‚Ä¢ùë¶= 0 if ùë§ùëáùë•< 0 \nPiecewise Linear model ùìó\nStill, w is the vector of parameters to be trained. But what is the optimization  objective?Or more formally, letwherestep(m) = 1, if m > 0 and step(m) = 0, otherwise\n\n--- Á¨¨ 27 È°µ ---\nLinear classification: natural attempt ‚Ä¢Given training data                                               i.i.d.from distribution ùê∑‚Ä¢Find                          that minimises ‚Ä¢Drawback: difficult to optimize‚Ä¢Non-differentiable‚Ä¢NP-hard in the worst case \n0-1 loss\nloss = 0, i.e., no loss, when the classification is the same as its label. loss =1, otherwise. \n--- Á¨¨ 28 È°µ ---\nLinear classification\nDrawback: not robust to ‚Äúoutliers‚Äù So, linear classification is probably not the right scheme for classificationGreen: logistic regression (to be introduced later)Magenta: linear classification\n--- Á¨¨ 29 È°µ ---\nlogistic regression\n--- Á¨¨ 30 È°µ ---\nWhy logistic regression?‚Ä¢Goal: The entire procedure of pursuing logistic regression in the below slides are to find output probabilities.Starting point‚Ä¢It's tempting to use the linear regression output as probabilities.‚Ä¢but it's a mistake because the output can be negative and greater than 1, whereas probability cannot. \n--- Á¨¨ 31 È°µ ---\nCompare the two \nLinear regression Linear classification \n--- Á¨¨ 32 È°µ ---\nBetween the two ‚Ä¢Prediction bounded in [0,1] ‚Ä¢Smooth ‚Ä¢Sigmoid: \n\n--- Á¨¨ 33 È°µ ---\nLinear regression: sigmoid prediction ‚Ä¢Squash the output of the linear function\nNew optimization objectiveFirst step\n--- Á¨¨ 34 È°µ ---\nLinear classification: logistic regression ‚Ä¢Is this the final? \nWe need a probability!If        is either 0 or 1, can we interpret                  as a probability value? \u0000(wTx(i))\n<latexit sha1_base64=\"txXsFv3w/N2Xk63kZig+7pS49WU=\">AAAB+3icbVDLTgIxFO34RHyNuHTTSExgQ2bQRJdENy4x4ZXAQDqlAw1tZ9J2FDKZX3HjQmPc+iPu/BsLzELBk9zk5Jx7c+89fsSo0o7zbW1sbm3v7Ob28vsHh0fH9kmhpcJYYtLEIQtlx0eKMCpIU1PNSCeSBHGfkbY/uZv77UciFQ1FQ88i4nE0EjSgGGkjDexCT9ERR6WnfmPaT0q0nJYHdtGpOAvAdeJmpAgy1Af2V28Y4pgToTFDSnVdJ9JegqSmmJE034sViRCeoBHpGioQJ8pLFren8MIoQxiE0pTQcKH+nkgQV2rGfdPJkR6rVW8u/ud1Yx3ceAkVUayJwMtFQcygDuE8CDikkmDNZoYgLKm5FeIxkghrE1fehOCuvrxOWtWKe1mpPlwVa7dZHDlwBs5BCbjgGtTAPaiDJsBgCp7BK3izUuvFerc+lq0bVjZzCv7A+vwBGO+T0g==</latexit>y(i)\n<latexit sha1_base64=\"xXljXSd6lMX9ueP8C+CEKdlwpjc=\">AAAB7nicbVBNSwMxEJ2tX7V+VT16CRahXspuFfRY9OKxgv2Adi3ZNNuGZpMlyQrL0h/hxYMiXv093vw3pu0etPXBwOO9GWbmBTFn2rjut1NYW9/Y3Cpul3Z29/YPyodHbS0TRWiLSC5VN8CaciZoyzDDaTdWFEcBp51gcjvzO09UaSbFg0lj6kd4JFjICDZW6qSPWZWdTwfliltz50CrxMtJBXI0B+Wv/lCSJKLCEI617nlubPwMK8MIp9NSP9E0xmSCR7RnqcAR1X42P3eKzqwyRKFUtoRBc/X3RIYjrdMosJ0RNmO97M3E/7xeYsJrP2MiTgwVZLEoTDgyEs1+R0OmKDE8tQQTxeytiIyxwsTYhEo2BG/55VXSrte8i1r9/rLSuMnjKMIJnEIVPLiCBtxBE1pAYALP8ApvTuy8OO/Ox6K14OQzx/AHzucP7wiPTQ==</latexit>Second step\n--- Á¨¨ 35 È°µ ---\nLinear classification: logistic regression ‚Ä¢A better approach: Interpret as a probability \nHere we assume that y=0 or y=1Pw(y=1|x)=\u0000(wTx)\n<latexit sha1_base64=\"8wfux9uKepHhAsBadmPWW29RysE=\">AAACAnicbVDLSsNAFJ3UV62vqCtxM1iEdlOSKuimUHTjskJf0MYwmU7aoZMHMxPbEIsbf8WNC0Xc+hXu/BunbRbaeuDC4Zx7ufceJ2RUSMP41jIrq2vrG9nN3Nb2zu6evn/QFEHEMWnggAW87SBBGPVJQ1LJSDvkBHkOIy1neD31W/eECxr4dRmHxPJQ36cuxUgqydaPavaoEFfMh3ERVmBX0L6HCqO7+rho63mjZMwAl4mZkjxIUbP1r24vwJFHfIkZEqJjGqG0EsQlxYxMct1IkBDhIeqTjqI+8oiwktkLE3iqlB50A67Kl3Cm/p5IkCdE7Dmq00NyIBa9qfif14mke2kl1A8jSXw8X+RGDMoATvOAPcoJlixWBGFO1a0QDxBHWKrUcioEc/HlZdIsl8yzUvn2PF+9SuPIgmNwAgrABBegCm5ADTQABo/gGbyCN+1Je9HetY95a0ZLZw7BH2ifP+mwldc=</latexit>Conditional probabilitySecond step\n--- Á¨¨ 36 È°µ ---\nLinear classification: logistic regression ‚Ä¢Find                          that minimises\nLogistic regression: MLE (maximum likelihood estimation) with sigmoidWhy log function used? To avoid numerical instability.unfold\nIncrease this one to make it as close as possible to 1Decrease this one to make it as close as possible to 0Third step\n--- Á¨¨ 37 È°µ ---\nLinear classification: logistic regression ‚Ä¢Given training data                                               i.i.d. from distribution ùê∑‚Ä¢Find w that minimises  \nNo close form solution; Need to use gradient descent, which will be introduced in the next lecture Achieved\n--- Á¨¨ 38 È°µ ---\nWhy sigmoid function? \n\n--- Á¨¨ 39 È°µ ---\nExercises\n‚Ä¢Given the dataset and consider the mean square root error, if we have the following two linear functions:‚Ä¢fw(x) = 2x1 + 1x2 + 20x3 - 330‚Ä¢fw(x) = 1x1 - 2x2 + 23x3 ‚Äì 332   please answer the following questions:‚Ä¢(1) which model is better for linear regression? ‚Ä¢(2) which model is better for linear classification    by considering 0-1 loss for yT=(0,1,1,0)? ‚Ä¢(3) which model is better for logistic regression for yT=(0,1,1,0)? ‚Ä¢(4) According to the logistic regression of the first model, what is the prediction result of the first model on a new input (181,92,12.4). x1x2x3y1828711.33251899212.33441787910.63501839012.7320Check the exercises of the lecture notesfor answer. "}