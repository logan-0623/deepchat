{"abstract": "**Abstract**  \n\u2022 PG-SAM integrates medical LLMs (Large Language Models) to enhance multi-organ segmentation accuracy  \n\u2022 Proposed fine-grained modality prior aligner bridges domain gaps between text and medical images  \n\u2022 Multi-level feature fusion and iterative mask optimizer improve boundary precision  \n\u2022 Achieves state-of-the-art performance on Synapse dataset with 84.79% mDice  \n\n**Introduction**  \n\u2022 Segment Anything Model (SAM) underperforms in medical imaging due to domain gaps  \n\u2022 Existing methods suffer from coarse text priors and misaligned modality fusion  \n\u2022 PG-SAM introduces medical LLMs for fine-grained anatomical text descriptions  \n\u2022 Key innovation: Joint optimization of semantic alignment and pixel-level boundary refinement  \n\n**Related Work**  \n\u2022 Prompt-free SAM variants (e.g., SAMed, H-SAM) reduce manual annotation dependency  \n\u2022 Visual-language models (VLMs) like CLIP provide cross-modal priors but lack medical specificity  \n\u2022 Medical LLMs (e.g., ClinicalBERT) offer domain expertise but require alignment with imaging  \n\u2022 Current limitations: Abstract semantics introduce noise in pixel-level segmentation  \n\n**Methodology**  \n\u2022 Fine-grained modality prior aligner:  \n  - Medical LLM generates anatomical text prompts $T \\in \\mathbb{R}^{d_{text}}$  \n  - LoRA-tuned CLIP encodes text-image similarity $W_s = \\sigma(P \\frac{F_{img}^\\top F_{text}}{\\|F_{img}\\|\\|F_{text}\\|})$  \n\u2022 Multi-level feature fusion:  \n  - Pyramid upsampling with deformable convolution: $F_{fusion} = \\phi(F_{up}) + \\psi(Align(G;\\theta))$  \n\u2022 Iterative mask optimizer:  \n  - Hypernetwork generates dynamic kernels $\\Omega_i = MLP(m_i) \\odot W_{base}$  \n  - Progressive refinement: $M_{t+1} = Clip(M_t + \\lambda \\Delta M_t)$  \n\n**Experiment**  \n\u2022 Synapse dataset: 3,779 CT slices across 8 abdominal organs  \n\u2022 10% annotation setting:  \n  - Achieves 75.75% mDice (+17.06% over AutoSAM)  \n  - HD95=12.35 (\u21935.68 vs. H-SAM)  \n\u2022 Fully supervised:  \n  - 84.79% mDice (\u21910.14% over SOTA)  \n  - Gallbladder segmentation \u21913.61%  \n\n**Conclusion**  \n\u2022 PG-SAM addresses SAM's medical domain limitations via three innovations  \n\u2022 Medical LLM priors reduce modality gap by 42% compared to base VLMs  \n\u2022 Unified pipeline improves mDice by 12% over prompt-free baselines  \n\u2022 Future work: Extend to 3D segmentation and multi-modal imaging"}